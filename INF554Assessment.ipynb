{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 Assessment: Low Rank Approximation and Denoising of Images\n",
    "\n",
    "\n",
    "In this assessment it is your task to use the PCA, MDS, SVD and NFM algorithms presented in Lecture 1 to obtain low rank approximations and denoise images.\n",
    "\n",
    "**Please submit a zip file containing the following to moodle by the 17th October 14:00 (Paris time):**\n",
    "\n",
    "- your filled out assessment jupyter notebook\n",
    "- a pdf containing your short written answers to the 5 questions in this notebook.\n",
    "\n",
    "Make sure to submit a couple of hours ahead of the deadline to ensure that technical difficulties do not cause you to miss the deadline. Late submissions will not be accepted.\n",
    "\n",
    "Please note that this assessment is *to be completed individually. We will forward detected cases of plagarism to the university,* which in serious cases can have farreaching consequences for you. So, please make sure to submit your own, original solutions to this assessment. \n",
    "\n",
    "*Disclaimer:* You will not receive marks for importing the pca, nmf or mds functions from any library. In this assessment we ask you to code these methods from scratch in the hope that this will give you a better understanding of them.\n",
    "\n",
    "\n",
    "## 1) Low Rank Approximation\n",
    "\n",
    "We begin by briefly summarising how the four methods used in this lab can be used to obtain low rank approximations of our data. In this lab we will be working with images, where each pixel is encoded by four values, namely RGBA (red, green, blue, alpha). These four dimensions encoding each pixel will be referred to as the four channels of our images. You should create low rank approximations separately for each channel, i.e., to obtain a low rank approxiamtion for the images, you should separately calculate the low rank approximation for each channel of the input image and then concatinate these low rank approximations. Therefore, in this lab our data  will be denoted by $X_i \\in R^{n\\times m}$ for $i \\in \\{R,G,B,A\\}.$\n",
    "\n",
    "\n",
    "\n",
    "**PCA:** Let $M_i\\in R^{n\\times m} $ be the matrix containing the column means of $X_i$ in each column. Further, let $C_i = X_i -  M_i$ denote the data matrix $X_i,$ where the column mean is substracted from each column. Furthermore, let $U_k\\in R^{m\\times k}$ denote the right singular vectors of $C_i$ corresponding to the largest $k$ singular values. Then, the PCA low-rank approximation, denoted $\\tilde{X}_i^{PCA},$ of $X_i$ is obtained as follows,\n",
    "\\begin{equation}\n",
    "\\tilde{X}_i^{PCA} = C_i U_k U_k^T + M.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**SVD:** Consider the singular value decomposition of $X_i = V \\Sigma W^T.$ Further, let $V_k \\in R^{n\\times k},$ $W_k \\in R^{m\\times k}$ and $\\Sigma_k \\in R^{k\\times k}$ contain the first $k$ left and right singular vectors and the largest $k$ singular values, respectivly. Then, the singular value low rank approximation, denoted $\\tilde{X}_i^{SVD},$ of $X_i$ is obtained as follows,\n",
    "\\begin{equation}\n",
    "\\tilde{X}_i^{SVD} = V_k \\Sigma_k W_k^T.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**NMF:** The non-negative matrix factorisation is iteratively calculated and directly produces a low rank approximation of $X_i$ into factors denoted by $A \\in R^{n \\times k}$ and $B\\in R^{k \\times m}.$ Hence,   \n",
    "\\begin{equation}\n",
    "\\tilde{X}_i^{NMF} = AB.\n",
    "\\end{equation}\n",
    "\n",
    "**MDS:** \n",
    "Let $M_i\\in R^{n\\times m} $ be the matrix containing the column means of $X_i$ in each column. Further, let $C_i = X_i -  M_i$ denote the data matrix $X_i,$ where the column mean is substracted from each column. \n",
    "For the multidimensional scaling algorithm we utilise the singular value decomposition of the matrix denoted $C_i C_i^T.$ Let $Q_k \\in R^{n\\times k}$ denote the matrix containing the $k$ singular vectors corresponding to the $k$ largest singluar values of $C_i C_i^T.$ Then, the low rank approximation, denoted $\\tilde{X}_i^{MDS},$ of $X_i$ is obtained as follows,\n",
    "\\begin{equation}\n",
    "\\tilde{X}_i^{MDS} =  Q_k Q_k^T C_i +M_i.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We begin with a theoretical question.\n",
    "\n",
    ">**Question 1: (4 Points)** The principal components produced in the PCA algorithm estimate $C_i U_{j}^\\star,$ where $U_{j}^\\star$ contains the $j^{\\mathrm{th}}$ eigenvector of the *true* covariance matrix ($Var[C_i]$) of our data. Proof that the true principal components $C_i U_{j}^\\star$ for $j \\in \\{1,\\ldots, m\\}$ are uncorrelated and relate their variance to the eigenvalues of the true covariance matrix. *Hint:* Consider $Var[C_i U_j^\\star]$ to answer this question. \n",
    "\n",
    "\n",
    "Now we load the image you will be working with in this assessement.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = mpimg.imread('IPPX_logo.png') \n",
    "plt.imshow(logo) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 1: (8 Points)** Complete the below functions to calculate low rank approximations of an input image with 4 channels using the PCA, SVD, NMF and MDS algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        k (int): the number of elements to be used in the low rank approximation \n",
    "    \n",
    "    Returns:\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "        U (np.array, dim: m,k,4): singular vectors used to obtain the low rank approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 1 here\n",
    "\n",
    "    \n",
    "    return X_low_rank_approx, U\n",
    "\n",
    "\n",
    "def svd(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        k (int): the number of elements to be used in the low rank approximation \n",
    "    \n",
    "    Returns:\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 1 here    \n",
    "\n",
    "    \n",
    "    return X_low_rank_approx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nmf(X, k, max_iter = 100, E = 10**(-4)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        k (int): the number of elements to be used in the low rank approximation \n",
    "    \n",
    "    Returns:\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 1 here    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return X_low_rank_approx\n",
    "\n",
    "\n",
    "\n",
    "def mds(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        k (int): the number of elements to be used in the low rank approximation \n",
    "    \n",
    "    Returns:\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 1 here    \n",
    "\n",
    "    \n",
    "    return X_low_rank_approx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 2: (1 Points)** Apply the functions implemented in Task 1 to produce low rank approximation images for $k$ equals 1, 5, 10, 30 and 60 of the logo and plot the results using the provided function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = [1,5,10,30,60]\n",
    "results_task2 = np.zeros(logo.shape + tuple([20]))\n",
    "\n",
    "\n",
    "#Please insert the code for Task 2 here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4,5, figsize=(8,10))\n",
    "\n",
    "\n",
    "titles = ['k = 1','k = 5','k = 10','k = 30','k = 60']\n",
    "methods = ['PCA', 'SVD', 'NMF', 'MDS']\n",
    "for i in range(4):\n",
    "    axs[i,0].set_ylabel(methods[i], fontsize=16,fontweight='bold')\n",
    "    for j in range(5):\n",
    "        if i==0:\n",
    "            axs[i,j].set_title(titles[j], fontweight='bold')\n",
    "        axs[i,j].imshow(np.clip(results_task2[:,:,:,i*5+j],0,1))\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "fig.tight_layout(pad=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 2: (2 Points)** Visually compare the low rank approximations and qualitatively assess (i) how the quality of the low rank approximatino varies as $k$ increases and (ii) which method produces the least lossy compression of the image. (No calculations or further processing are required here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the PCA reconstruction is often evaluated by calculating the fraction of the total variance of the image that is preserved in the reconstructed image. Hence, the first evaluation metric we want to use to assess the quality of the reconstruction is the sum of the eigenvalues of the estimated covariance matrix $(\\tilde{C}_i^{h})^T \\tilde{C}_i^{h}/n$ for $h \\in \\{\\text{PCA, SVD, NMF, MDS}\\}$ divided by the sum of eigenvalues of the estimated covariance matrix of the original image $C_i^T C_i/m.$ The final metric is produced by averaging the fraction of the sum of eigenvalues over all four image channels.\n",
    "\n",
    "The MDS algorithm minimises the *Stress* metric, which is calculated as follows,\n",
    "\\begin{equation}\n",
    "    Stress = \\sqrt{\\dfrac{\\sum_{a=1}^n \\sum_{b=1}^n (d_{ab} - \\delta_{ab})^2}{\\sum_{a=1}^n \\sum_{b=1}^n \\delta_{ab}^2}},\n",
    "\\end{equation} \n",
    "where $d_{ab}$ the Euclidian distance between rows $a$ and $b$ of matrix $\\tilde{X}_i^{h}$ for $h \\in \\{\\text{PCA, SVD, NMF, MDS}\\}$ and $\\delta_{ab}$ contains the Euclidian distance between rows $a$ and $b$ of $X_i$. Again the final metric is produced by averaging the Stress metric over all four image channels.\n",
    "\n",
    "The performance of the SVD and NMF reconstruction is commonly measured using the difference of the original image and the reconstructed image in Frobenius norm. This will be the third metric we will consider to compare our low rank approximations.\n",
    "\n",
    ">**Task 3: (7 Points)** Fill out the code to calculate the three presented evaluation metrics of our methods. Then use these functions to quanititatively assess the approximation error of our four methods when $k=10.$ Fill your results into the provided table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evals(X, X_low_rank_approx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    Returns:\n",
    "        eval_fraction (float): the ratio of the explained variance as described in the problem description\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 3 here\n",
    "\n",
    "    \n",
    "    return eval_fraction\n",
    "\n",
    "def stress(X, X_low_rank_approx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    Returns:\n",
    "        stress (float): the stress metric value\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 3 here\n",
    "    \n",
    "    \n",
    "    return stress\n",
    "\n",
    "\n",
    "def Fnorm_difference(X, X_low_rank_approx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, dim: n,m,4): containing the image channel to be processed\n",
    "        X_low_rank_approx (np.array, dim: n,m,4): low rank approximation of X\n",
    "    Returns:\n",
    "        diff (float): the difference of the two inputs in Frobenius norm\n",
    "    \"\"\"\n",
    "    \n",
    "    #Please insert the code for Task 3 here\n",
    "\n",
    "    \n",
    "    return diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_task3 = np.zeros(shape=[4,3])\n",
    "\n",
    "#Please insert the code for Task 3 here to fill the results matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = ['evals', 'stress', 'norm']\n",
    "methods = ['PCA', 'SVD', 'NMF', 'MDS']\n",
    "\n",
    "print('\\t',end='')\n",
    "for j in range(3):\n",
    "    print(metrics[j], end='\\t')\n",
    "print('')\n",
    "for i in range(4):\n",
    "    print(methods[i], end='\\t')\n",
    "    for j in range(3):\n",
    "        print(np.round(results_task3[i,j],4),'\\t', end='')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Orthogonal Transformation of the Input Data\n",
    "\n",
    "Now we will work with another image, which was obtained from the logo we have been working with so far, by applying a random orthogonal transformation to the pixel values. Hence, if $X_i$ contains the pixel values in the $i^{\\mathrm{th}}$ channel of the original image and  $X_i'$ contains the pixel values in the $i^{\\mathrm{th}}$ channel of the transformed image, then they are related as follows,\n",
    "$$\n",
    "X_i' = X_i Q \\qquad ~ \\qquad \\forall i,\n",
    "$$\n",
    "where $Q$ is an orthogonal matrix which was randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transformed_image.pickle', 'rb') as f:\n",
    "    transformed_image = pickle.load(f)\n",
    "    \n",
    "    \n",
    "plt.imshow(np.clip(transformed_image,0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 4: (2 Points)** Repeat task 3 on the transformed image. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task4 = np.zeros(shape=[4,3])\n",
    "\n",
    "#Please insert the code for Task 4 here to fill the results matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = ['evals', 'stress', 'norm']\n",
    "methods = ['PCA', 'SVD', 'NMF', 'MDS']\n",
    "\n",
    "print('\\t',end='')\n",
    "for j in range(3):\n",
    "    print(metrics[j], end='\\t')\n",
    "print('')\n",
    "for i in range(4):\n",
    "    print(methods[i], end='\\t')\n",
    "    for j in range(3):\n",
    "        print(np.round(results_task4[i,j],4),'\\t', end='')\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 3: (4 Points)** Prove that the low rank approximations of $X_i$ and $X_i'$ obtained using the PCA algorithm capture equal amounts of the variance of the two images. Furthermore, proof that the principal components ($C_i U_{jk},$ and $C_i' U_{jk}'$ for $j\\in \\{1,\\ldots, k\\})$ are equal. (Note that in practice you might encounter principal components which are unequal since the obtained eigenvectors have opposing signs. This should not be a problem for you here since this question should be answered using a theoretical pen and paper proof.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Image Denoising\n",
    "\n",
    "In this section we will use the eigenvectors used in the PCA of the original logo to denoise two noisy images of the logo. In the cell below we load the two noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('noisy_image1.pickle', 'rb') as f:\n",
    "    noisy_image1 = pickle.load(f)\n",
    "    \n",
    "with open('noisy_image2.pickle', 'rb') as f:\n",
    "    noisy_image2 = pickle.load(f)\n",
    "    \n",
    "fig, axs = plt.subplots(1,2, figsize=(15,5))  \n",
    "axs[0].imshow(np.clip(noisy_image1,0,1))\n",
    "axs[1].imshow(np.clip(noisy_image2,0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: (6 Points)** Use the eigenvectors used in the PCA algorithm on the original image to obtain low rank approximations of the noisy images. Use the number of eigenvectors which minmises the distance of the denoised image and the original logo in the Frobenius norm metric introduced in Section 1). Make sure to enter the number of used eigenvectors in the plot titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please insert the code for Task 5 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_image1 = np.zeros(logo.shape)\n",
    "denoised_image2 = np.zeros(logo.shape)\n",
    "\n",
    "\n",
    "\n",
    "n,m,D = logo.shape\n",
    "k1_optimal = #Please insert the code for Task 5 here\n",
    "k2_optimal = #Please insert the code for Task 5 here\n",
    "\n",
    "#Please insert the code for Task 5 here\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,5, figsize=(15,5))    \n",
    "axs[0].imshow(logo)\n",
    "axs[1].imshow(np.clip(noisy_image1,0,1))\n",
    "axs[2].imshow(np.clip(denoised_image1,0,1))\n",
    "axs[3].imshow(np.clip(noisy_image2,0,1))\n",
    "axs[4].imshow(np.clip(denoised_image2,0,1))\n",
    "\n",
    "\n",
    "\n",
    "titles = ['original', 'noisy logo 1', 'k= %d'%(k1_optimal), 'noisy logo 2', 'k= %d'%(k2_optimal)]\n",
    "for i in range(5):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_title(titles[i], fontweight='bold')\n",
    "fig.tight_layout(pad=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: (2 Points)** Comment on how the low rank approximation affects the level of noise in the images and explain the observed change in the level of noise visible in the low rank approximations. (We don't expect you to write more than 4 sentences in this answer.) \n",
    "\n",
    "**Question 5: (4 Points)** Are you able to draw conclusions about the structural properties of the noise added to the two images from your success in denoising them?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
